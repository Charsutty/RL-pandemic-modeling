{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7531e7c2",
   "metadata": {},
   "source": [
    "# Contrôle optimal d'une pandémie par apprentissage par renforcement tabulaire\n",
    "\n",
    "**Modèle SEIRD** — Résolution par **Q-Learning**, **SARSA** et **DP approchée** sur état réduit $(S, E, I)$ discrétisé.\n",
    "\n",
    "L'environnement simule la dynamique SEIRD complète (voir `model.py`), mais l'agent observe uniquement $(S, E, I)$ discrétisé en bins non-uniformes. Les compartiments $R$ et $D$ n'influencent pas la dynamique utile ni les coûts.\n",
    "\n",
    "**Contrôles :**\n",
    "- $u_{conf} \\in \\{0, 0.25, 0.5, 0.75, 1\\}$ — niveau de confinement\n",
    "- $u_{vacc} \\in \\{0, 0.25, 0.5, 0.75, 1\\}$ — intensité de vaccination (×max_vacc_rate)\n",
    "\n",
    "→ 25 actions discrètes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e432344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramètres épidémiologiques : β=0.27, σ=0.14, γ=0.1, μ=0.01\n",
      "R₀ = β/γ = 2.70\n",
      "Horizon = 365 jours, dt = 1.0\n",
      "\n",
      "Coûts (normalisés C_eco=1) :\n",
      "  C_eco=1.0, C_vacc=0.005, C_vie=1000.0, C_hosp=3.0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import SEIREnv, ProblemConfig, SocioEconomicConfig\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (14, 5),\n",
    "    \"axes.grid\": True,\n",
    "    \"font.size\": 11,\n",
    "})\n",
    "\n",
    "config = ProblemConfig()\n",
    "print(f\"Paramètres épidémiologiques : β={config.beta}, σ={config.sigma}, γ={config.gamma}, μ={config.mu}\")\n",
    "print(f\"R₀ = β/γ = {config.beta / config.gamma:.2f}\")\n",
    "print(f\"Horizon = {config.max_steps} jours, dt = {config.dt}\")\n",
    "print(f\"\\nCoûts (normalisés C_eco=1) :\")\n",
    "c = config.socio_eco_config\n",
    "print(f\"  C_eco={c.confinement_eco_cost}, C_vacc={c.vaccination_eco_cost}, \"\n",
    "      f\"C_vie={c.life_cost}, C_hosp={c.infection_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d4ee4",
   "metadata": {},
   "source": [
    "## 1. Discrétisation de l'espace d'états et d'actions\n",
    "\n",
    "**État réduit :** On ne garde que $(S, E, I)$ car les équations de transition de $S$, $E$, $I$ ne dépendent ni de $R$ ni de $D$. C'est Markovien pour la partie utile à la décision.\n",
    "\n",
    "**Bins non-uniformes :** Plus fins près de 0 pour $E$ et $I$ (là où se jouent les transitions critiques).\n",
    "\n",
    "- $S$ : 11 bins — $[0, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.93, 0.95, 0.97, 0.985, 1.001]$\n",
    "- $E, I$ : 11 bins — $[0, 10^{-5}, 3{\\times}10^{-5}, 10^{-4}, 3{\\times}10^{-4}, 10^{-3}, 3{\\times}10^{-3}, 10^{-2}, 3{\\times}10^{-2}, 0.1, 0.3, 1.001]$\n",
    "\n",
    "→ $11^3 = 1331$ états possibles (en pratique moins car $S+E+I \\leq 1$).\n",
    "\n",
    "**Actions :** Grille $5 \\times 5$ → 25 actions discrètes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2733b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Discrétisation SEI + grille d'actions\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "S_bins = np.array([0, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.93, 0.95, 0.97, 0.985, 1.001])\n",
    "E_bins = np.array([0, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 0.1, 0.3, 1.001])\n",
    "I_bins = E_bins.copy()\n",
    "\n",
    "nS = len(S_bins) - 1  # 11\n",
    "nE = len(E_bins) - 1  # 11\n",
    "nI = len(I_bins) - 1  # 11\n",
    "n_states = nS * nE * nI\n",
    "print(f\"Nombre d'états discrétisés : {nS}×{nE}×{nI} = {n_states}\")\n",
    "\n",
    "\n",
    "def bin_index(x: float, bins: np.ndarray) -> int:\n",
    "    \"\"\"Retourne l'indice du bin contenant x (0-indexé).\"\"\"\n",
    "    return int(np.clip(np.digitize([x], bins)[0] - 1, 0, len(bins) - 2))\n",
    "\n",
    "\n",
    "def state_to_id(obs: np.ndarray) -> int:\n",
    "    \"\"\"Convertit une observation [S, E, I, ...] en un indice entier.\"\"\"\n",
    "    iS = bin_index(float(obs[0]), S_bins)\n",
    "    iE = bin_index(float(obs[1]), E_bins)\n",
    "    iI = bin_index(float(obs[2]), I_bins)\n",
    "    return (iS * nE + iE) * nI + iI\n",
    "\n",
    "\n",
    "# Grille d'actions : 5 niveaux × 2 contrôles → 25 actions\n",
    "grid_vals = np.array([0.0, 0.25, 0.5, 0.75, 1.0], dtype=np.float32)\n",
    "actions = np.array([(uc, uv) for uc in grid_vals for uv in grid_vals], dtype=np.float32)\n",
    "n_actions = len(actions)\n",
    "print(f\"Nombre d'actions discrètes : {n_actions}\")\n",
    "print(f\"Exemples d'actions : {actions[:5].tolist()} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Fonctions utilitaires : simulation & enregistrement\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_episode(env, policy_fn, record=True):\n",
    "    \"\"\"\n",
    "    Simule un épisode complet en utilisant policy_fn(obs) → action continue [u_conf, u_vacc].\n",
    "    Retourne le reward total et optionnellement l'historique complet.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    history = {\n",
    "        \"S\": [], \"E\": [], \"I\": [], \"R\": [], \"D\": [],\n",
    "        \"u_conf\": [], \"u_vacc\": [],\n",
    "        \"reward\": [],\n",
    "        \"L_eco\": [], \"L_vacc\": [], \"L_deaths\": [], \"L_infection\": [],\n",
    "    }\n",
    "\n",
    "    while not done:\n",
    "        action = policy_fn(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        if record:\n",
    "            history[\"S\"].append(obs[0])\n",
    "            history[\"E\"].append(obs[1])\n",
    "            history[\"I\"].append(obs[2])\n",
    "            history[\"R\"].append(obs[3])\n",
    "            history[\"D\"].append(obs[4])\n",
    "            history[\"u_conf\"].append(float(action[0]))\n",
    "            history[\"u_vacc\"].append(float(action[1]))\n",
    "            history[\"reward\"].append(reward)\n",
    "            history[\"L_eco\"].append(info.get(\"L_eco\", 0))\n",
    "            history[\"L_vacc\"].append(info.get(\"L_vacc\", 0))\n",
    "            history[\"L_deaths\"].append(info.get(\"L_deaths\", 0))\n",
    "            history[\"L_infection\"].append(info.get(\"L_infection\", 0))\n",
    "\n",
    "    if record:\n",
    "        for k in history:\n",
    "            history[k] = np.array(history[k])\n",
    "    return total_reward, history\n",
    "\n",
    "\n",
    "def tabular_policy_fn(Q_table, actions_grid):\n",
    "    \"\"\"Fabrique une policy_fn greedy à partir d'une Q-table.\"\"\"\n",
    "    def _policy(obs):\n",
    "        s = state_to_id(obs)\n",
    "        a_idx = int(np.argmax(Q_table[s]))\n",
    "        return actions_grid[a_idx]\n",
    "    return _policy\n",
    "\n",
    "print(\"Fonctions utilitaires chargées ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d2fb1",
   "metadata": {},
   "source": [
    "## 2. Q-Learning (off-policy, ε-greedy)\n",
    "\n",
    "Mise à jour classique :\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big( r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\Big)$$\n",
    "\n",
    "avec décroissance exponentielle de $\\varepsilon$ (exploration → exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Q-Learning tabulaire\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "env = SEIREnv(config)\n",
    "Q_ql = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "\n",
    "alpha_ql = 0.1\n",
    "gamma_ql = 0.99\n",
    "eps_start, eps_end = 1.0, 0.05\n",
    "n_episodes_ql = 5000\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "rewards_ql = []\n",
    "\n",
    "for ep in range(n_episodes_ql):\n",
    "    obs, _ = env.reset()\n",
    "    s = state_to_id(obs)\n",
    "\n",
    "    eps = eps_end + (eps_start - eps_end) * np.exp(-ep / 1000)\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        # ε-greedy\n",
    "        if rng.random() < eps:\n",
    "            a = rng.integers(n_actions)\n",
    "        else:\n",
    "            a = int(np.argmax(Q_ql[s]))\n",
    "\n",
    "        obs2, r, terminated, truncated, _ = env.step(actions[a])\n",
    "        s2 = state_to_id(obs2)\n",
    "\n",
    "        # Mise à jour Q-learning (off-policy : max sur s')\n",
    "        Q_ql[s, a] += alpha_ql * (r + gamma_ql * np.max(Q_ql[s2]) - Q_ql[s, a])\n",
    "\n",
    "        s = s2\n",
    "        ep_reward += r\n",
    "        done = terminated or truncated\n",
    "\n",
    "    rewards_ql.append(ep_reward)\n",
    "\n",
    "    if (ep + 1) % 1000 == 0:\n",
    "        mean_r = np.mean(rewards_ql[-200:])\n",
    "        print(f\"Q-Learning — épisode {ep+1}/{n_episodes_ql}, ε={eps:.3f}, \"\n",
    "              f\"reward moyen (200 derniers) = {mean_r:.1f}\")\n",
    "\n",
    "print(f\"\\nQ-Learning terminé. Reward final moyen = {np.mean(rewards_ql[-200:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd780ecf",
   "metadata": {},
   "source": [
    "## 3. SARSA (on-policy, ε-greedy)\n",
    "\n",
    "Mise à jour :\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big( r + \\gamma \\, Q(s',a') - Q(s,a) \\Big)$$\n",
    "\n",
    "Contrairement au Q-Learning, SARSA utilise l'action $a'$ **effectivement choisie** (pas le max). Cela rend l'algorithme plus conservateur — il tient compte de l'exploration résiduelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  SARSA tabulaire (on-policy)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "Q_sarsa = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "\n",
    "alpha_s = 0.1\n",
    "gamma_s = 0.99\n",
    "n_episodes_sarsa = 5000\n",
    "\n",
    "rewards_sarsa = []\n",
    "\n",
    "for ep in range(n_episodes_sarsa):\n",
    "    obs, _ = env.reset()\n",
    "    s = state_to_id(obs)\n",
    "\n",
    "    eps = eps_end + (eps_start - eps_end) * np.exp(-ep / 1000)\n",
    "\n",
    "    # Choisir a₀\n",
    "    if rng.random() < eps:\n",
    "        a = rng.integers(n_actions)\n",
    "    else:\n",
    "        a = int(np.argmax(Q_sarsa[s]))\n",
    "\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        obs2, r, terminated, truncated, _ = env.step(actions[a])\n",
    "        s2 = state_to_id(obs2)\n",
    "\n",
    "        # Choisir a' ε-greedy (on-policy)\n",
    "        if rng.random() < eps:\n",
    "            a2 = rng.integers(n_actions)\n",
    "        else:\n",
    "            a2 = int(np.argmax(Q_sarsa[s2]))\n",
    "\n",
    "        # Mise à jour SARSA (on-policy : utilise Q(s', a'))\n",
    "        Q_sarsa[s, a] += alpha_s * (r + gamma_s * Q_sarsa[s2, a2] - Q_sarsa[s, a])\n",
    "\n",
    "        s = s2\n",
    "        a = a2\n",
    "        ep_reward += r\n",
    "        done = terminated or truncated\n",
    "\n",
    "    rewards_sarsa.append(ep_reward)\n",
    "\n",
    "    if (ep + 1) % 1000 == 0:\n",
    "        mean_r = np.mean(rewards_sarsa[-200:])\n",
    "        print(f\"SARSA — épisode {ep+1}/{n_episodes_sarsa}, ε={eps:.3f}, \"\n",
    "              f\"reward moyen (200 derniers) = {mean_r:.1f}\")\n",
    "\n",
    "print(f\"\\nSARSA terminé. Reward final moyen = {np.mean(rewards_sarsa[-200:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979283d5",
   "metadata": {},
   "source": [
    "## 4. Courbes d'apprentissage Q-Learning vs SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a83d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Courbes d'apprentissage (reward moyen glissant)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "window = 100\n",
    "\n",
    "def smooth(x, w=window):\n",
    "    return np.convolve(x, np.ones(w)/w, mode=\"valid\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.plot(smooth(rewards_ql), label=\"Q-Learning\", alpha=0.9)\n",
    "ax.plot(smooth(rewards_sarsa), label=\"SARSA\", alpha=0.9)\n",
    "ax.set_xlabel(\"Épisode\")\n",
    "ax.set_ylabel(f\"Reward moyen (fenêtre glissante {window})\")\n",
    "ax.set_title(\"Courbes d'apprentissage — Q-Learning vs SARSA\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0a2fb",
   "metadata": {},
   "source": [
    "## 5. DP approchée (Value Iteration backward, horizon fini)\n",
    "\n",
    "Pour chaque pas de temps $t = T, T{-}1, \\ldots, 1$, on approxime :\n",
    "$$V_t(s) = \\max_a \\Big( \\hat{\\mathbb{E}}[r(s,a,S')] + \\gamma \\, \\hat{\\mathbb{E}}[V_{t+1}(S')] \\Big)$$\n",
    "\n",
    "L'espérance est estimée par $K$ simulations Monte-Carlo depuis un état représentatif du bin $s$. C'est de la **DP approchée** car l'espace d'état continu est discrétisé et les transitions sont estimées empiriquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b236c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  DP approchée — Value Iteration backward (horizon fini)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "T = config.max_steps  # 365\n",
    "K = 5                 # Nombre de simulations MC par (s, a) pour estimer la transition\n",
    "gamma_dp = 0.99\n",
    "\n",
    "# On construit un état représentatif (milieu du bin) pour chaque bin\n",
    "def bin_center(bins, idx):\n",
    "    return 0.5 * (bins[idx] + bins[idx + 1])\n",
    "\n",
    "# Tables de valeurs et politique : V[t, s], pi[t, s]\n",
    "V_dp = np.zeros((T + 1, n_states), dtype=np.float64)\n",
    "pi_dp = np.zeros((T, n_states), dtype=np.int32)\n",
    "\n",
    "# On ne traite que les états plausibles (S+E+I ≈ ≤ 1)\n",
    "plausible = []\n",
    "for iS in range(nS):\n",
    "    for iE in range(nE):\n",
    "        for iI in range(nI):\n",
    "            s_val = bin_center(S_bins, iS)\n",
    "            e_val = bin_center(E_bins, iE)\n",
    "            i_val = bin_center(I_bins, iI)\n",
    "            if s_val + e_val + i_val <= 1.05:\n",
    "                sid = (iS * nE + iE) * nI + iI\n",
    "                plausible.append((sid, s_val, e_val, i_val))\n",
    "\n",
    "print(f\"États plausibles : {len(plausible)} / {n_states}\")\n",
    "\n",
    "# Backward induction\n",
    "for t in range(T - 1, -1, -1):\n",
    "    for sid, s_val, e_val, i_val in plausible:\n",
    "        best_val = -np.inf\n",
    "        best_a = 0\n",
    "\n",
    "        for a_idx in range(n_actions):\n",
    "            total_r = 0.0\n",
    "            total_v_next = 0.0\n",
    "\n",
    "            for _ in range(K):\n",
    "                env_dp = SEIREnv(config)\n",
    "                env_dp.reset()\n",
    "                # Placer l'env dans l'état représentatif\n",
    "                r_val = max(0, 1.0 - s_val - e_val - i_val)\n",
    "                env_dp.state = np.array([s_val, e_val, i_val, r_val, 0.0], dtype=np.float32)\n",
    "                env_dp.current_step = t\n",
    "\n",
    "                obs2, r, _, _, _ = env_dp.step(actions[a_idx])\n",
    "                s2 = state_to_id(obs2)\n",
    "\n",
    "                total_r += r\n",
    "                total_v_next += V_dp[t + 1, s2]\n",
    "\n",
    "            mean_r = total_r / K\n",
    "            mean_v = total_v_next / K\n",
    "            q_val = mean_r + gamma_dp * mean_v\n",
    "\n",
    "            if q_val > best_val:\n",
    "                best_val = q_val\n",
    "                best_a = a_idx\n",
    "\n",
    "        V_dp[t, sid] = best_val\n",
    "        pi_dp[t, sid] = best_a\n",
    "\n",
    "    if (T - t) % 50 == 0:\n",
    "        print(f\"DP backward : t={t}, states traités = {len(plausible)}\")\n",
    "\n",
    "print(\"DP approchée terminée ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08049e",
   "metadata": {},
   "source": [
    "## 6. Heuristiques de référence (baselines)\n",
    "\n",
    "Pour évaluer nos agents RL et DP, on les compare à des politiques simples :\n",
    "\n",
    "1. **Aucun contrôle** : $u_{conf}=0$, $u_{vacc}=0$ (laissez-faire)\n",
    "2. **Vaccination max** : $u_{conf}=0$, $u_{vacc}=1$ (vaccination à plein régime, pas de confinement)\n",
    "3. **Confinement seuil** : confiner fort quand $I > 2\\%$, vacciner à 50%\n",
    "4. **Double seuil adaptatif** : confinement modulé + vaccination décroissante avec $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Définition des heuristiques\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def policy_no_control(obs):\n",
    "    \"\"\"Aucun contrôle.\"\"\"\n",
    "    return np.array([0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "def policy_vacc_max(obs):\n",
    "    \"\"\"Vaccination max, pas de confinement.\"\"\"\n",
    "    return np.array([0.0, 1.0], dtype=np.float32)\n",
    "\n",
    "def policy_seuil_conf(obs):\n",
    "    \"\"\"Confinement fort si I > 2%, vaccination à 50%.\"\"\"\n",
    "    I = float(obs[2])\n",
    "    u_conf = 1.0 if I > 0.02 else 0.0\n",
    "    return np.array([u_conf, 0.5], dtype=np.float32)\n",
    "\n",
    "def policy_double_seuil(obs):\n",
    "    \"\"\"Double seuil adaptatif.\"\"\"\n",
    "    S, E, I = float(obs[0]), float(obs[1]), float(obs[2])\n",
    "    # Confinement modulé\n",
    "    if I > 0.02:\n",
    "        u_conf = 0.75\n",
    "    elif I > 0.005:\n",
    "        u_conf = 0.25\n",
    "    else:\n",
    "        u_conf = 0.0\n",
    "    # Vaccination : forte quand S est grand (beaucoup de gens à protéger)\n",
    "    u_vacc = min(1.0, S) if S > 0.3 else 0.25\n",
    "    return np.array([u_conf, u_vacc], dtype=np.float32)\n",
    "\n",
    "# Politique DP (time-dependent)\n",
    "def policy_dp(obs, env_ref=None):\n",
    "    \"\"\"Politique DP approchée (dépend du step courant).\"\"\"\n",
    "    s = state_to_id(obs)\n",
    "    t = min(env_ref.current_step, T - 1) if env_ref else 0\n",
    "    a_idx = pi_dp[t, s]\n",
    "    return actions[a_idx]\n",
    "\n",
    "print(\"Heuristiques définies ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce10193",
   "metadata": {},
   "source": [
    "## 7. Évaluation comparative — Trajectoires et coûts\n",
    "\n",
    "On simule un épisode complet avec chaque politique et on compare :\n",
    "- Évolution $(S, E, I, R, D)$\n",
    "- Actions ($u_{conf}$, $u_{vacc}$)\n",
    "- Coûts cumulés par composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b3c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Évaluation de toutes les politiques\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "env_eval = SEIREnv(config)\n",
    "results = {}\n",
    "\n",
    "# --- Heuristiques ---\n",
    "for name, pol_fn in [\n",
    "    (\"Aucun contrôle\", policy_no_control),\n",
    "    (\"Vaccination max\", policy_vacc_max),\n",
    "    (\"Seuil confinement\", policy_seuil_conf),\n",
    "    (\"Double seuil\", policy_double_seuil),\n",
    "]:\n",
    "    r, h = run_episode(env_eval, pol_fn)\n",
    "    results[name] = h\n",
    "    print(f\"{name:25s} → reward total = {r:10.1f}\")\n",
    "\n",
    "# --- Q-Learning greedy ---\n",
    "r, h = run_episode(env_eval, tabular_policy_fn(Q_ql, actions))\n",
    "results[\"Q-Learning\"] = h\n",
    "print(f\"{'Q-Learning':25s} → reward total = {r:10.1f}\")\n",
    "\n",
    "# --- SARSA greedy ---\n",
    "r, h = run_episode(env_eval, tabular_policy_fn(Q_sarsa, actions))\n",
    "results[\"SARSA\"] = h\n",
    "print(f\"{'SARSA':25s} → reward total = {r:10.1f}\")\n",
    "\n",
    "# --- DP approchée ---\n",
    "env_dp_eval = SEIREnv(config)\n",
    "obs_dp, _ = env_dp_eval.reset()\n",
    "done_dp = False\n",
    "h_dp = {k: [] for k in [\"S\",\"E\",\"I\",\"R\",\"D\",\"u_conf\",\"u_vacc\",\"reward\",\n",
    "                          \"L_eco\",\"L_vacc\",\"L_deaths\",\"L_infection\"]}\n",
    "total_r_dp = 0.0\n",
    "while not done_dp:\n",
    "    action_dp = policy_dp(obs_dp, env_dp_eval)\n",
    "    obs_dp, r_dp, term_dp, trunc_dp, info_dp = env_dp_eval.step(action_dp)\n",
    "    done_dp = term_dp or trunc_dp\n",
    "    total_r_dp += r_dp\n",
    "    h_dp[\"S\"].append(obs_dp[0]); h_dp[\"E\"].append(obs_dp[1])\n",
    "    h_dp[\"I\"].append(obs_dp[2]); h_dp[\"R\"].append(obs_dp[3])\n",
    "    h_dp[\"D\"].append(obs_dp[4])\n",
    "    h_dp[\"u_conf\"].append(float(action_dp[0]))\n",
    "    h_dp[\"u_vacc\"].append(float(action_dp[1]))\n",
    "    h_dp[\"reward\"].append(r_dp)\n",
    "    h_dp[\"L_eco\"].append(info_dp.get(\"L_eco\",0))\n",
    "    h_dp[\"L_vacc\"].append(info_dp.get(\"L_vacc\",0))\n",
    "    h_dp[\"L_deaths\"].append(info_dp.get(\"L_deaths\",0))\n",
    "    h_dp[\"L_infection\"].append(info_dp.get(\"L_infection\",0))\n",
    "\n",
    "for k in h_dp:\n",
    "    h_dp[k] = np.array(h_dp[k])\n",
    "results[\"DP approchée\"] = h_dp\n",
    "print(f\"{'DP approchée':25s} → reward total = {total_r_dp:10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8d59d",
   "metadata": {},
   "source": [
    "### 7a. Évolution de la population (S, E, I, R, D) — toutes politiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Plot : Évolution des compartiments SEIRD par politique\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "policy_names = list(results.keys())\n",
    "n_pol = len(policy_names)\n",
    "compartments = [\"S\", \"E\", \"I\", \"R\", \"D\"]\n",
    "colors_comp = {\"S\": \"tab:blue\", \"E\": \"tab:orange\", \"I\": \"tab:red\", \"R\": \"tab:green\", \"D\": \"tab:gray\"}\n",
    "\n",
    "fig, axes = plt.subplots(n_pol, 1, figsize=(14, 3.5 * n_pol), sharex=True)\n",
    "\n",
    "for i, name in enumerate(policy_names):\n",
    "    ax = axes[i]\n",
    "    h = results[name]\n",
    "    days = np.arange(len(h[\"S\"]))\n",
    "    for c in compartments:\n",
    "        ax.plot(days, h[c], label=c, color=colors_comp[c], linewidth=1.5)\n",
    "    ax.set_ylabel(\"Proportion\")\n",
    "    ax.set_title(name, fontweight=\"bold\")\n",
    "    ax.legend(loc=\"center right\", fontsize=9)\n",
    "    ax.set_ylim(-0.02, 1.02)\n",
    "\n",
    "axes[-1].set_xlabel(\"Jours\")\n",
    "fig.suptitle(\"Évolution des compartiments SEIRD selon la politique\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2db98",
   "metadata": {},
   "source": [
    "### 7b. Commandes appliquées ($u_{conf}$, $u_{vacc}$) au cours du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Plot : Actions u_conf et u_vacc par politique\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(n_pol, 2, figsize=(14, 3 * n_pol), sharex=True)\n",
    "\n",
    "for i, name in enumerate(policy_names):\n",
    "    h = results[name]\n",
    "    days = np.arange(len(h[\"u_conf\"]))\n",
    "\n",
    "    # u_conf\n",
    "    axes[i, 0].step(days, h[\"u_conf\"], where=\"post\", color=\"tab:red\", linewidth=1.2)\n",
    "    axes[i, 0].set_ylabel(\"$u_{conf}$\")\n",
    "    axes[i, 0].set_ylim(-0.05, 1.05)\n",
    "    axes[i, 0].set_title(f\"{name} — Confinement\", fontsize=10)\n",
    "\n",
    "    # u_vacc\n",
    "    axes[i, 1].step(days, h[\"u_vacc\"], where=\"post\", color=\"tab:green\", linewidth=1.2)\n",
    "    axes[i, 1].set_ylabel(\"$u_{vacc}$\")\n",
    "    axes[i, 1].set_ylim(-0.05, 1.05)\n",
    "    axes[i, 1].set_title(f\"{name} — Vaccination\", fontsize=10)\n",
    "\n",
    "axes[-1, 0].set_xlabel(\"Jours\")\n",
    "axes[-1, 1].set_xlabel(\"Jours\")\n",
    "fig.suptitle(\"Commandes appliquées au cours du temps\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76718bc6",
   "metadata": {},
   "source": [
    "### 7c. Comparaison de la courbe épidémique (I) et des décès (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Plot : Comparaison I(t) et D(t) entre toutes les politiques\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "cmap = plt.cm.Set1\n",
    "for i, name in enumerate(policy_names):\n",
    "    h = results[name]\n",
    "    days = np.arange(len(h[\"I\"]))\n",
    "    ls = \"--\" if name in [\"Aucun contrôle\", \"Vaccination max\"] else \"-\"\n",
    "    ax1.plot(days, h[\"I\"], label=name, color=cmap(i / n_pol), linestyle=ls, linewidth=1.5)\n",
    "    ax2.plot(days, h[\"D\"], label=name, color=cmap(i / n_pol), linestyle=ls, linewidth=1.5)\n",
    "\n",
    "ax1.set_title(\"Courbe épidémique I(t)\")\n",
    "ax1.set_xlabel(\"Jours\"); ax1.set_ylabel(\"Proportion d'infectés\")\n",
    "ax1.legend(fontsize=8)\n",
    "\n",
    "ax2.set_title(\"Décès cumulés D(t)\")\n",
    "ax2.set_xlabel(\"Jours\"); ax2.set_ylabel(\"Proportion de décès\")\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67c57a",
   "metadata": {},
   "source": [
    "### 7d. Coûts cumulés par composante ($\\mathcal{L}_{eco}$, $\\mathcal{L}_{vacc}$, $\\mathcal{L}_{deaths}$, $\\mathcal{L}_{hosp}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Plot : Coûts cumulés par composante (barres empilées)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "cost_keys = [\"L_eco\", \"L_vacc\", \"L_deaths\", \"L_infection\"]\n",
    "cost_labels = [\"$\\\\mathcal{L}_{eco}$\", \"$\\\\mathcal{L}_{vacc}$\",\n",
    "               \"$\\\\mathcal{L}_{deaths}$\", \"$\\\\mathcal{L}_{hosp}$\"]\n",
    "cost_colors = [\"tab:blue\", \"tab:green\", \"tab:gray\", \"tab:red\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(n_pol)\n",
    "width = 0.6\n",
    "bottoms = np.zeros(n_pol)\n",
    "\n",
    "cost_totals = {name: {} for name in policy_names}\n",
    "\n",
    "for j, (ck, cl, cc) in enumerate(zip(cost_keys, cost_labels, cost_colors)):\n",
    "    vals = []\n",
    "    for name in policy_names:\n",
    "        cum = float(np.sum(results[name][ck]))\n",
    "        vals.append(cum)\n",
    "        cost_totals[name][cl] = cum\n",
    "    vals = np.array(vals)\n",
    "    ax.bar(x, vals, width, bottom=bottoms, label=cl, color=cc, alpha=0.85)\n",
    "    bottoms += vals\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(policy_names, rotation=25, ha=\"right\")\n",
    "ax.set_ylabel(\"Coût cumulé (unités $C_{eco}$·jours)\")\n",
    "ax.set_title(\"Décomposition des coûts cumulés par politique\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau récapitulatif\n",
    "print(f\"\\n{'Politique':25s} | {'L_eco':>10s} | {'L_vacc':>10s} | {'L_deaths':>10s} | {'L_hosp':>10s} | {'TOTAL':>10s}\")\n",
    "print(\"-\" * 85)\n",
    "for name in policy_names:\n",
    "    h = results[name]\n",
    "    le = np.sum(h[\"L_eco\"]); lv = np.sum(h[\"L_vacc\"])\n",
    "    ld = np.sum(h[\"L_deaths\"]); li = np.sum(h[\"L_infection\"])\n",
    "    print(f\"{name:25s} | {le:10.1f} | {lv:10.1f} | {ld:10.1f} | {li:10.1f} | {le+lv+ld+li:10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d72551",
   "metadata": {},
   "source": [
    "## 8. Étude de sensibilité ($C_{vacc}$, $C_{hosp}$)\n",
    "\n",
    "On ré-entraîne Q-Learning avec des multiplicateurs ×0.5, ×1, ×2 sur $C_{vacc}$ et $C_{hosp}$ pour observer l'impact sur la politique optimale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Sensibilité : Q-Learning rapide avec différents C_vacc, C_hosp\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "multipliers = [0.5, 1.0, 2.0]\n",
    "sensitivity_results = {}\n",
    "\n",
    "for m_vacc in multipliers:\n",
    "    for m_hosp in multipliers:\n",
    "        label = f\"C_vacc×{m_vacc}, C_hosp×{m_hosp}\"\n",
    "        print(f\"Entraînement {label} ...\", end=\" \")\n",
    "\n",
    "        sec = SocioEconomicConfig(\n",
    "            vaccination_eco_cost=0.005 * m_vacc,\n",
    "            infection_cost=3.0 * m_hosp,\n",
    "        )\n",
    "        cfg_sens = ProblemConfig(socio_eco_config=sec)\n",
    "        env_sens = SEIREnv(cfg_sens)\n",
    "\n",
    "        Q_sens = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "        n_ep_sens = 2000\n",
    "\n",
    "        for ep in range(n_ep_sens):\n",
    "            obs, _ = env_sens.reset()\n",
    "            s = state_to_id(obs)\n",
    "            eps = eps_end + (eps_start - eps_end) * np.exp(-ep / 500)\n",
    "            done = False\n",
    "            while not done:\n",
    "                if rng.random() < eps:\n",
    "                    a = rng.integers(n_actions)\n",
    "                else:\n",
    "                    a = int(np.argmax(Q_sens[s]))\n",
    "                obs2, r, terminated, truncated, _ = env_sens.step(actions[a])\n",
    "                s2 = state_to_id(obs2)\n",
    "                Q_sens[s, a] += 0.1 * (r + 0.99 * np.max(Q_sens[s2]) - Q_sens[s, a])\n",
    "                s = s2\n",
    "                done = terminated or truncated\n",
    "\n",
    "        # Évaluer\n",
    "        env_eval_sens = SEIREnv(cfg_sens)\n",
    "        r_sens, h_sens = run_episode(env_eval_sens, tabular_policy_fn(Q_sens, actions))\n",
    "        sensitivity_results[label] = h_sens\n",
    "        print(f\"reward = {r_sens:.1f}\")\n",
    "\n",
    "print(\"\\nÉtude de sensibilité terminée ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "#  Plot : Sensibilité — Courbes I(t) et commandes\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, m_vacc in enumerate(multipliers):\n",
    "    for j, m_hosp in enumerate(multipliers):\n",
    "        ax = axes[i, j]\n",
    "        label = f\"C_vacc×{m_vacc}, C_hosp×{m_hosp}\"\n",
    "        h = sensitivity_results[label]\n",
    "        days = np.arange(len(h[\"I\"]))\n",
    "\n",
    "        ax.plot(days, h[\"I\"], color=\"tab:red\", label=\"I(t)\", linewidth=1.5)\n",
    "        ax.plot(days, h[\"u_conf\"], color=\"tab:blue\", label=\"$u_{conf}$\", alpha=0.6, linewidth=1)\n",
    "        ax.plot(days, h[\"u_vacc\"], color=\"tab:green\", label=\"$u_{vacc}$\", alpha=0.6, linewidth=1)\n",
    "\n",
    "        ax.set_title(f\"$C_{{vacc}}$×{m_vacc}, $C_{{hosp}}$×{m_hosp}\", fontsize=10)\n",
    "        if i == 0 and j == 0:\n",
    "            ax.legend(fontsize=7)\n",
    "\n",
    "axes[2, 1].set_xlabel(\"Jours\")\n",
    "axes[1, 0].set_ylabel(\"Proportion / Intensité\")\n",
    "fig.suptitle(\"Sensibilité de la politique Q-Learning à $C_{vacc}$ et $C_{hosp}$\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0232539e",
   "metadata": {},
   "source": [
    "## 9. Synthèse et discussion\n",
    "\n",
    "**Méthodes implémentées :**\n",
    "1. **Q-Learning** (off-policy, ε-greedy) — exploite le meilleur Q(s', a') futur\n",
    "2. **SARSA** (on-policy, ε-greedy) — plus conservateur, tient compte de l'exploration\n",
    "3. **DP approchée** (backward induction, horizon fini) — baseline optimale (en théorie)\n",
    "4. **Heuristiques** — aucun contrôle, vaccination max, seuils simples/adaptatifs\n",
    "\n",
    "**Observations attendues :**\n",
    "- Le **laissez-faire** produit un pic épidémique rapide et beaucoup de décès\n",
    "- La **vaccination seule** est insuffisante (0.5%/jour = trop lent face à R₀=2.7)\n",
    "- Le **Q-Learning / SARSA** apprend un compromis confinement + vaccination\n",
    "- La **DP approchée** sert de référence mais est limitée par la discrétisation grossière\n",
    "- L'étude de sensibilité montre que ↑$C_{hosp}$ → confinement plus agressif, ↑$C_{vacc}$ → vaccination plus tardive\n",
    "\n",
    "**Limites :**\n",
    "- Discrétisation en bins → perte de précision (mais adapté au cadre tabulaire \"cours\")\n",
    "- Pas de réseau de neurones (choix délibéré : méthodes tabulaires pures)\n",
    "- Bruit très faible dans l'env → quasi-déterministe\n",
    "\n",
    "**Références :** voir `report/projet.tex`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
